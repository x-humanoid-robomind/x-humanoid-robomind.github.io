<!DOCTYPE html>
<html>

<head>

  <meta charset="utf-8">
  <meta name="description"
    content="RoboMIND: Establishing a Benchmark for Multi-embodiment Intelligence Normative Data in Robot Manipulation.">
  <meta name="keywords" content="RoboMIND, Multi-embodiment Intelligence, Robot Manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>RoboMIND: Benchmark on Multi-embodiment Intelligence Normative Data for Robot Manipulation</title>


  <script>
    window.dataLayer = window.dataLayer || [];
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <!-- <link href="./static/css/googlecss.css"
        rel="stylesheet"> -->
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="./static/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="./static/js/jquerymin351.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <style>
    .author-block {
        display: none;
    }

    .publication-authors:hover .author-block {
        display: block;
    }
</style>

</head>

<body>

<!--  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://x-humanoid.com">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>
      </div>

    </div>
  </nav> -->

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">RoboMIND: Benchmark on Multi-embodiment
              Intelligence Normative Data for Robot Manipulation </h1>
              <!-- 新增的接收声明 - 字体加大并加粗 -->
            <div class="is-size-5 has-text-weight-bold has-text-centered" 
                 style="margin-top: 0.8rem; margin-bottom: 1.2rem;">
              Accepted by Robotics: Science and Systems (RSS) 2025
            </div>
            <div class="is-size-5 publication-authors">
                <span class="team-name"><b>RoboMIND Team <p style="font-size: 70%">(hover to display full author list)</p></b></span>
                <span class="author-block">Kun Wu<sup>1,∗</sup>, Chengkai Hou<sup>2,3,∗</sup>, Jiaming Liu<sup>2,3,∗</sup>,
                Zhengping Che<sup>1,∗,†</sup>, Xiaozhu Ju<sup>1,∗,†</sup>,<span class="author-block"></span> Zhuqin Yang<sup>1</sup>, Meng Li<sup>1</sup>, Yinuo Zhao<sup>1</sup>, Zhiyuan Xu<sup>1</sup>, Guang Yang<sup>1</sup>, Shichao Fan<sup>1</sup>, Xinhua Wang<sup>1</sup>, Fei Liao<sup>1</sup>, Zhen Zhao<sup>1</sup>, Guangyu Li<sup>1</sup>, Zhao Jin<sup>1</sup>, Lecheng Wang<sup>1</sup>, Jilei Mao<sup>1</sup>, Ning Liu<sup>1</sup>, Pei Ren<sup>1</sup>, Qiang Zhang<sup>1</sup>, Yaoxu Lyu<sup>2</sup>, Mengzhen Liu<sup>2,3</sup>, Jingyang He<sup>2,3</sup>, Yulin Luo<sup>2,3</sup>, Zeyu Gao<sup>3</sup>, Chenxuan Li<sup>2</sup>, Chenyang Gu<sup>2,3</sup>, Yankai Fu<sup>2</sup>,<br> Di Wu<sup>2</sup>, Xingyu Wang<sup>2</sup>, Sixiang Chen<sup>2,3</sup>, Zhenyu Wang<sup>2</sup>, Pengju An<sup>2,3</sup>, Siyuan Qian<sup>2,3</sup>,
                 <br> Shanghang Zhang<sup>2,3
                    <span class="icon">
                      <svg class="svg-inline--fa fa-envelope fa-w-16" aria-hidden="true" focusable="false"
                        data-prefix="fa" data-icon="envelope" role="img" xmlns="http://www.w3.org/2000/svg"
                        viewBox="0 0 512 512" data-fa-i2svg="">
                        <path fill="currentColor"
                          d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z">
                        </path>
                      </svg>
                    
                    </span>
                  </sup>
                , Jian Tang<sup>1
                    <span class="icon">
                      <svg class="svg-inline--fa fa-envelope fa-w-16" aria-hidden="true" focusable="false"
                        data-prefix="fa" data-icon="envelope" role="img" xmlns="http://www.w3.org/2000/svg"
                        viewBox="0 0 512 512" data-fa-i2svg="">
                        <path fill="currentColor"
                          d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z">
                        </path>
                      </svg>
                    </span>
                  </sup>
                </span>


                <div class="is-size-6 publication-authors"> 
                  <span class="author-block"><sup>*</sup>Co-first Authors, 
                    <sup> <svg class="svg-inline--fa fa-envelope fa-w-16" aria-hidden="true" focusable="false"
                      data-prefix="fa" data-icon="envelope" role="img" xmlns="http://www.w3.org/2000/svg"
                      viewBox="0 0 512 512" data-fa-i2svg="">
                      <path fill="currentColor"
                        d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z">
                      </path>
                    </svg>
                    </sup> Corresponding Authors, 
                    <sup>†</sup> Project Leaders 
                  </span>
                 <br>
               
                <div class="is-size-6 publication-authors id=institute">
                  <sup>1</sup>Beijing Innovation Center of Humanoid Robotics, <p>
                    <sup>2</sup>State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University, <p></p>
                    <sup>3</sup>Beijing Academy of Artificial Intelligence</p>

                </div>
                 <br> 
                 <img src="./static/images/logonew.png" border=0 width=80%> 
                  
                </div>
              </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2412.13877" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/2412.13877" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span> -->
                <!-- Code Link. -->
                <!-- <span class="link-block">
                  <a href="https://github.com/x-humanoid-robomind"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://data.flopsera.com/data-detail/21181956226031626?type=open"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Data</span>
                  </a>
                  <span class="link-block">
                    <a href="https://huggingface.co/datasets/x-humanoid-robomind/RoboMIND"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-database"></i>
                      </span>
                      <span>Huggingface</span>
                    </a>
                  <span class="link-block">
                    <a href="https://github.com/x-humanoid-robomind/x-humanoid-robomind.github.io"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Github</span>
                    </a>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/teaser.mp4" type="video/mp4">
        </video> -->
        <div class="yush-div-center">
          <img src="./static/images/roboMIND_new.png" class="img-responsive">
        </div>

        <h2 class="subtitle has-text-centered">
          We introduce <span style="font-weight: bold"> RoboMIND</span>, a benchmark on Multi-embodiment Intelligence Normative Data for Robot Manipulation, comprising 107k real-world demonstration trajectories across 4 embodiments, 479 diverse tasks and 96 distinct object classes.
        </h2>
      </div>
    </div>
  </section>

  <section class="section" id="single-task-1">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Developing robust and general-purpose manipulation policies is a key goal in robotics. To achieve effective
              generalization, it is essential to construct comprehensive datasets
              that encompass a large number of demonstration trajectories
              and diverse tasks. While existing works have focused on assembling various
              individual robot datasets, there is still a lack of a unified data collection standard and insufficient high-quality data across diverse
              tasks, scenarios, and robot types. In this paper, we introduce
              RoboMIND (Multi-embodiment Intelligence Normative Data for
              Robot Manipulation), a dataset containing <span style="font-weight: bold"> 107k </span> demonstration
              trajectories across <span style="font-weight: bold"> 479 </span> diverse tasks involving <span style="font-weight: bold"> 96 </span> object classes.
              RoboMIND is collected through human teleoperation and encompasses comprehensive robotic-related information, including
              multi-view observations, proprioceptive robot state information,
              and linguistic task descriptions. To ensure data consistency and
              reliability for imitation learning, RoboMIND is built on a unified
              data collection platform and a standardized protocol, covering
              four distinct robotic embodiments: the Franka Emika Panda,
              the UR5e, the AgileX dual-arm robot, and a humanoid robot
              with dual dexterous hands. Our dataset also includes 5k real-
              world failure demonstrations, each accompanied by detailed
              causes, enabling failure reflection and correction during policy
              learning. Additionally, we created a digital twin environment
              in the Isaac Sim simulator, replicating the real-world tasks
              and assets, which facilitates the low-cost collection of additional
              training data and enables efficient evaluation. To demonstrate
              the quality and diversity of our dataset, we conducted extensive
              experiments using various imitation learning methods for single-task settings and state-of-the-art Vision-Language-Action (VLA)
              models for multi-task scenarios. By leveraging RoboMIND, the
              VLA models achieved high manipulation success rates and
              demonstrated strong generalization capabilities. To the best of
              our knowledge, RoboMIND is the largest multi-embodiment
              teleoperation dataset collected on a unified platform, providing
              large-scale and high-quality robotic training data. 
            </p>

          </div>
        </div>
      </div>
      <br>
      <br>
      <!--/ Abstract. -->

     <!-- <div class="columns is-centered">
        <div class="column is-full-width">
          <video id="teaser" autoplay muted loop playsinline height="100%" controls>
            <source src="./static/videos/teaser.mp4" type="video/mp4">
          </video>
          <br>
          <br>
        </div>
      </div> -->

      <!-- Hardware Setup. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Hardware Setup</h2>
          <div class="content has-text-justified">
            <div class="columns" id="4pic">
              <div class="column has-text-centered">
                  <img src="./static/images/Franka.png" class="interpolation-image" alt="" style="display: block; width: 100%; margin-left: auto; margin-right: auto">
              </div>
              <div class="column has-text-centered">
                  <img src="./static/images/Tien Kung.png" class="interpolation-image" alt="" style="display: block; width: 100%; margin-left: auto; margin-right: auto">
              </div>
              <div class="column has-text-centered">
                <img src="./static/images/AgileX.png" class="interpolation-image" alt="" style="display: block; width: 100%; margin-left: auto; margin-right: auto">
            </div>
            <div class="column has-text-centered">
                  <img src="./static/images/UR.png" class="interpolation-image" alt="" style="display: block; width: 100%; margin-left: auto; margin-right: auto">
              </div>
          </div>

            <p> Here are some examples of the hardware setup for data collection, which is also the hardware setup we used to establish our real-world experiments. For the Franka Emika Panda robots, we use cameras positioned at the top, left, and right viewpoints to record the visual information of the task trajectories. For the AgileX/Tien Kung robots, we use their built-in cameras to record visual information. For UR robots, we use an external top camera. All demonstrations are collected using high-quality human teleoperation and stored on a unified intelligence platform. </p>

          </div>
        </div>
      </div>
      <br>
      <br>
      <!-- / Hardware Setup. -->

      <!-- RoboMIND Data Analysis. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">RoboMIND Data Analysis</h2>
          <div class="content has-text-justified">
            <div class="yush-div-center">
              <img src="./static/images/piechart_new.png" class="img-responsive">
            </div>

            <p><b>Dataset Overview</b>. (a) the total trajectory numbers categorized by different types of robots, (b) average trajectory lengths (frames) categorized by different types of robots, (c) trajectory ratio of different task categories (Artic. M.: Articulated Manipulations; Coord. M.: Coordination Manipulations; Basic M.: Basic Manipulations; Obj. Int.: Multiple Object Interactions; Precision M.: Precision Manipulations; Scene U.: Scene Understanding), and (d) trajectory ratio of different scenarios.</p>
            <br>
            <br>
            <div class="yush-div-center" style="display: flex; justify-content: center;">
               <img src="./static/images/Distribution_new.png" class="img-responsive" style="width: 99%"> 
              </div> 
            <p> <b>Distribution of objects</b> in RoboMIND, categorized as domestic, industrial, kitchen, office, and retail.</p>
            <br>
            <br>
            <div class="yush-div-center" style="display: flex; justify-content: center;">
              <img src="./static/images/skill_counts_new.png" class="img-responsive" style="width: 95%">
            </div>
              <p> <b>Left</b>: <b>Skill number</b> distribution histogram across tasks for each embodiment. We observe that over 70% of the Franka tasks involve only a single skill, while over 75% of the Tien Kung and AgileX tasks involve two or more skills, indicating that these dual-arm tasks are mostly long-horizon tasks. <b>Right</b>: We visualize the AX-PutCarrot task with the AgileX robot, which involves three different skills. </p>
              <br>
              <br>
              <div class="yush-div-center" style="display: flex; justify-content: center;">  
            <img src="./static/images/Annotation.png" class="img-responsive">
           </div>
              <p> <b>Language Description Annotation</b>. We provide refined linguistic annotations for 10,000 successful robot motion trajectories. The video of the robotic arm placing the apple in the drawer is divided into six segments using Gemini. The language descriptions provided for each segment were initially generated by Gemini and subsequently refined through manual revision.  </p>
              <br>
              <br>
            <div class="yush-div-center" style="display: flex; justify-content: center;">     
              <img src="./static/images/Visualization.png" class="img-responsive">
            </div>
              <p> <b>Visualization of failed data collection cases</b>. We present two examples of failure from Franka and AgileX. In the FR-PlacePlateInPlateRack task (second row), the Franka arm fails to align with the slot, causing the plate to slip due to operator interference. In the AX-PutCarrot task (fourth row), the AgileX gripper unexpectedly opens, dropping the carrot. These failure cases were filtered out during quality inspection to maintain dataset quality. </p>
          </div>
        </div>
      </div>
      <!-- / RoboMIND Data Analysis. -->
  </section>

  <section class="section" id="demo">
    <div class="container is-max-desktop">
      <!-- Experiment  -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Experiments</h2>
      <!-- Experiment  -->
      <p> We conduct comprehensive experiments employing four popular imitation learning methods, including ACT, Diffusion Policy,
        BAKU, RDT-1B, Crossformer and OpenVLA on selected RoboMIND tasks to assess their performance and limitations.
      </p>
      <br>

      <div id="single-task">
        <!-- Performance on Single Tasks.. -->
         <br>
        <h3 class="title is-4">Success Examples of ACT on Single Tasks</h3> 
        <!-- <p> <b>Single Task Results</b>. ACT achieves an average success rate of 30.7% (Franka), 34.0% (Tien Kung), 55.3% (AgileX) and 38.0% (UR-5e).
        </p> -->
        <div class="content has-text-justified">
            <div class="columns">
              <div class="column has-text-centered">
                  <p style="font-size: 125%"><b>FR-PlacePearBowl</b></p>
                  <video poster="" id="" autoplay="" controls="" muted="" loop="" height="100%" playbackrate="2.0" style="border-radius: 5px;">
                      <source src="./static/videos/compressed_FR-PlacePearBowl.MP4" type="video/mp4">
                  </video>
              </div>
              <div class="column has-text-centered">
                  <p style="font-size: 125%"><b>FR-SideCloseDrawer</b></p>
                  <video poster="" id="" autoplay="" controls="" muted="" loop="" height="100%" playbackrate="2.0" style="border-radius: 5px;">
                      <source src="./static/videos/compressed_FR-SideCloseDrawer.MP4" type="video/mp4">
                  </video>
              </div>
              <div class="column has-text-centered">
                  <p style="font-size: 125%"><b>FR-PlaceBluePink</b></p>
                  <video poster="" id="" autoplay="" controls="" muted="" loop="" height="100%" playbackrate="2.0" style="border-radius: 5px;">
                      <source src="./static/videos/FR-PlaceBluePink.mp4" type="video/mp4">
                  </video>
              </div>
          </div>
          <div class="columns">
              <div class="column has-text-centered">
                  <p style="font-size: 125%"><b>HR-OpenTrashBin</b></p>
                  <video poster="" id="" autoplay="" controls="" muted="" loop="" height="100%" playbackrate="2.0" style="border-radius: 5px;">
                      <source src="./static/videos/TK-OpenTrashBin.mp4" type="video/mp4">
                  </video>
              </div>
              <div class="column has-text-centered">
                  <p style="font-size: 125%"><b>HR-CloseTrashBin</b></p>
                  <video poster="" id="" autoplay="" controls="" muted="" loop="" height="100%" playbackrate="2.0" style="border-radius: 5px;">
                      <source src="./static/videos/TK-CloseTrashBin.mp4" type="video/mp4">
                  </video>
              </div>
              <div class="column has-text-centered">
                  <p style="font-size: 125%"><b>HR-OpenDrawerLowerCabinet</b></p>
                  <video poster="" id="" autoplay="" controls="" muted="" loop="" height="100%" playbackrate="2.0" style="border-radius: 5px;">
                      <source src="./static/videos/TK-OpenDrawerLowerCabinet.mp4" type="video/mp4">
                  </video>
              </div>
          </div>
          <div class="columns">
            <div class="column has-text-centered">
                <p style="font-size: 125%"><b>AX-AppleYellowPlate</b></p>
                <video poster="" id="" autoplay="" controls="" muted="" loop="" height="100%" playbackrate="2.0" style="border-radius: 5px;">
                    <source src="./static/videos/AX-AppleYellowPlate.mp4" type="video/mp4">
                </video>
            </div>
            <div class="column has-text-centered">
                <p style="font-size: 125%"><b>AX-CarrotGreenPlate</b></p>
                <video poster="" id="" autoplay="" controls="" muted="" loop="" height="100%" playbackrate="2.0" style="border-radius: 5px;">
                    <source src="./static/videos/AX-CarrotGreenPlate.mp4" type="video/mp4">
                </video>
            </div>
            <div class="column has-text-centered">
                <p style="font-size: 125%"><b>AX-UnpackBowl</b></p>
                <video poster="" id="" autoplay="" controls="" muted="" loop="" height="100%" playbackrate="2.0" style="border-radius: 5px;">
                    <source src="./static/videos/AX-UnpackBowl.mp4" type="video/mp4">
                </video>
            </div>
          </div>
        <div class="columns">
          <div class="column has-text-centered">
              <p style="font-size: 125%"><b>UR-CloseTopWhiteDrawer</b></p>
              <video poster="" id="" autoplay="" controls="" muted="" loop="" height="100%" playbackrate="2.0" style="border-radius: 5px;">
                  <source src="./static/videos/UR-CloseTopWhiteDrawer.mp4" type="video/mp4">
              </video>
          </div>
          <div class="column has-text-centered">
              <p style="font-size: 125%"><b>UR-PickRoundBread</b></p>
              <video poster="" id="" autoplay="" controls="" muted="" loop="" height="100%" playbackrate="2.0" style="border-radius: 5px;">
                  <source src="./static/videos/UR-PickRoundBread.mp4" type="video/mp4">
              </video>
          </div>
          <div class="column has-text-centered">
              <p style="font-size: 125%"><b>AX-PackPlate</b></p>
              <video poster="" id="" autoplay="" controls="" muted="" loop="" height="100%" playbackrate="2.0" style="border-radius: 5px;">
                  <source src="./static/videos/AX-PackPlate.mp4" type="video/mp4">
              </video>
          </div>
         </div>
          <div class="columns is-centered">
          </div>
          <br>
        </div>
      </div>
      <br>
      <div id="multitask">
        <!-- Performance on Multi-Tasks. -->
        <h3 class="title is-4">Success Examples of RDT-1B Finetuned on Multi-task Setting</h3>
        <div class="content has-text-justified">
          <div class="columns">
            <div class="column has-text-centered">
                <p style="font-size: 125%"><b>AX-AppleBluePlate</b></p>
                <video poster="" id="" autoplay="" controls="" muted="" loop="" height="100%" playbackrate="2.0" style="border-radius: 5px;">
                    <source src="./static/videos/AX-AppleBluePlate.mp4" type="video/mp4">
                </video>
            </div>
            <div class="column has-text-centered">
                <p style="font-size: 125%"><b>AX-PackBowl</b></p>
                <video poster="" id="" autoplay="" controls="" muted="" loop="" height="100%" playbackrate="2.0" style="border-radius: 5px;">
                    <source src="./static/videos/AX-PackBowl.mp4" type="video/mp4">
                </video>
            </div>
            <div class="column has-text-centered">
                <p style="font-size: 125%"><b>AX-TakePotato</b></p>
                <video poster="" id="" autoplay="" controls="" muted="" loop="" height="100%" playbackrate="2.0" style="border-radius: 5px;">
                    <source src="./static/videos/AX-TakePotato.mp4" type="video/mp4">
                </video>
            </div>
        </div>
        <div class="columns is-centered">
        </div>
        <br>
        
        
        <h3 class="title is-4">Success Examples of OpenVLA Finetuned on Multi-task Setting</h3>
        <div class="content has-text-justified">
        <div class="columns">
            <div class="column has-text-centered">
                <p style="font-size: 125%"><b>FR-OpenCapLid</b></p>
                <video poster="" id="" autoplay="" controls="" muted="" loop="" height="100%" playbackrate="2.0" style="border-radius: 5px;">
                    <source src="./static/videos/FR-OpenCapLid.mp4" type="video/mp4">
                </video>
            </div>
            <div class="column has-text-centered">
                <p style="font-size: 125%"><b>FR-PickStrawberryInBowl</b></p>
                <video poster="" id="" autoplay="" controls="" muted="" loop="" height="100%" playbackrate="2.0" style="border-radius: 5px;">
                    <source src="./static/videos/FR-PickStrawberryInBowl.mp4" type="video/mp4">
                </video>
            </div>
            <div class="column has-text-centered">
                <p style="font-size: 125%"><b>FR-SlideCloseDrawer</b></p>
                <video poster="" id="" autoplay="" controls="" muted="" loop="" height="100%" playbackrate="2.0" style="border-radius: 5px;">
                    <source src="./static/videos/compressed_FR-SideCloseDrawer.MP4" type="video/mp4">
                </video>
            </div>
          </div>
          <div class="columns is-centered">
          </div>
          <br>
          <p>
          For more details on data analysis and experiment results, please refer to our <a href="https://arxiv.org/abs/2412.13877">paper</a>. For technical questions or any other inquiries, please file a bug at <a href="https://github.com/x-humanoid-robomind/x-humanoid-robomind.github.io">the github repo</a>.

          </p>          
        </div>
        </div>
      </div>
  </section>



  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{wu2025robomind,
              title={Robomind: Benchmark on multi-embodiment intelligence normative data for robot manipulation},
              author={Wu, Kun and Hou, Chengkai and Liu, Jiaming and Che, Zhengping and Ju, Xiaozhu and Yang, Zhuqin and Li, Meng and Zhao, Yinuo and Xu, Zhiyuan and Yang, Guang and others},
              booktitle={Robotics: Science and Systems (RSS) 2025}, 
              year={2025},
              publisher={Robotics: Science and Systems Foundation}, 
              url={https://www.roboticsproceedings.org/rss21/p152.pdf} 
}
      </code></pre>
    </div>
    <br>
  </section>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://arxiv.org/abs/2412.13877">
          <i class="fas fa-file-pdf"></i>
        </a>
        <!-- <a class="icon-link" href="https://x-humanoid-robomind.github.io/" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a> -->
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content has-text-centered">
            <p>
              The website template was borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>
              and <a href="https://eureka-research.github.io/">Eureka</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
